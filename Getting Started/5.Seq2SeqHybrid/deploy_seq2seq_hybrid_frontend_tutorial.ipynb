{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Deploying a Seq2Seq Model with the Hybrid Frontend\n",
    "==================================================\n",
    "**Author:** `Matthew Inkawhich <https://github.com/MatthewInkawhich>`_\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tutorial will walk through the process of transitioning a\n",
    "sequence-to-sequence model to Torch Script using PyTorch’s Hybrid\n",
    "Frontend. The model that we will convert is the chatbot model from the\n",
    "`Chatbot tutorial <https://pytorch.org/tutorials/beginner/chatbot_tutorial.html>`__. \n",
    "You can either treat this tutorial as a “Part 2” to the Chatbot tutorial\n",
    "and deploy your own pretrained model, or you can start with this\n",
    "document and use a pretrained model that we host. In the latter case,\n",
    "you can reference the original Chatbot tutorial for details\n",
    "regarding data preprocessing, model theory and definition, and model\n",
    "training.\n",
    "\n",
    "What is the Hybrid Frontend?\n",
    "----------------------------\n",
    "\n",
    "During the research and development phase of a deep learning-based\n",
    "project, it is advantageous to interact with an **eager**, imperative\n",
    "interface like PyTorch’s. This gives users the ability to write\n",
    "familiar, idiomatic Python, allowing for the use of Python data\n",
    "structures, control flow operations, print statements, and debugging\n",
    "utilities. Although the eager interface is a beneficial tool for\n",
    "research and experimentation applications, when it comes time to deploy\n",
    "the model in a production environment, having a **graph**-based model\n",
    "representation is very beneficial. A deferred graph representation\n",
    "allows for optimizations such as out-of-order execution, and the ability\n",
    "to target highly optimized hardware architectures. Also, a graph-based\n",
    "representation enables framework-agnostic model exportation. PyTorch\n",
    "provides mechanisms for incrementally converting eager-mode code into\n",
    "Torch Script, a statically analyzable and optimizable subset of Python\n",
    "that Torch uses to represent deep learning programs independently from\n",
    "the Python runtime.\n",
    "\n",
    "The API for converting eager-mode PyTorch programs into Torch Script is\n",
    "found in the torch.jit module. This module has two core modalities for\n",
    "converting an eager-mode model to a Torch Script graph representation:\n",
    "**tracing** and **scripting**. The ``torch.jit.trace`` function takes a\n",
    "module or function and a set of example inputs. It then runs the example\n",
    "input through the function or module while tracing the computational\n",
    "steps that are encountered, and outputs a graph-based function that\n",
    "performs the traced operations. **Tracing** is great for straightforward\n",
    "modules and functions that do not involve data-dependent control flow,\n",
    "such as standard convolutional neural networks. However, if a function\n",
    "with data-dependent if statements and loops is traced, only the\n",
    "operations called along the execution route taken by the example input\n",
    "will be recorded. In other words, the control flow itself is not\n",
    "captured. To convert modules and functions containing data-dependent\n",
    "control flow, a **scripting** mechanism is provided. Scripting\n",
    "explicitly converts the module or function code to Torch Script,\n",
    "including all possible control flow routes. To use script mode, be sure\n",
    "to inherit from the the ``torch.jit.ScriptModule`` base class (instead\n",
    "of ``torch.nn.Module``) and add a ``torch.jit.script`` decorator to your\n",
    "Python function or a ``torch.jit.script_method`` decorator to your\n",
    "module’s methods. The one caveat with using scripting is that it only\n",
    "supports a restricted subset of Python. For all details relating to the\n",
    "supported features, see the Torch Script `language\n",
    "reference <https://pytorch.org/docs/master/jit.html>`__. To provide the\n",
    "maximum flexibility, the modes of Torch Script can be composed to\n",
    "represent your whole program, and these techniques can be applied\n",
    "incrementally.\n",
    "\n",
    ".. figure:: /_static/img/chatbot/pytorch_workflow.png\n",
    "   :align: center\n",
    "   :alt: workflow\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Acknowledgements\n",
    "----------------\n",
    "\n",
    "This tutorial was inspired by the following sources:\n",
    "\n",
    "1) Yuan-Kuei Wu’s pytorch-chatbot implementation:\n",
    "   https://github.com/ywk991112/pytorch-chatbot\n",
    "\n",
    "2) Sean Robertson’s practical-pytorch seq2seq-translation example:\n",
    "   https://github.com/spro/practical-pytorch/tree/master/seq2seq-translation\n",
    "\n",
    "3) FloydHub’s Cornell Movie Corpus preprocessing code:\n",
    "   https://github.com/floydhub/textutil-preprocess-cornell-movie-corpus\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare Environment\n",
    "-------------------\n",
    "\n",
    "First, we will import the required modules and set some constants. If\n",
    "you are planning on using your own model, be sure that the\n",
    "``MAX_LENGTH`` constant is set correctly. As a reminder, this constant\n",
    "defines the maximum allowed sentence length during training and the\n",
    "maximum length output that the model is capable of producing.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "from __future__ import unicode_literals\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import re\n",
    "import os\n",
    "import unicodedata\n",
    "import numpy as np\n",
    "\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "\n",
    "MAX_LENGTH = 10  # Maximum sentence length\n",
    "\n",
    "# Default word tokens\n",
    "PAD_token = 0  # Used for padding short sentences\n",
    "SOS_token = 1  # Start-of-sentence token\n",
    "EOS_token = 2  # End-of-sentence token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Overview\n",
    "--------------\n",
    "\n",
    "As mentioned, the model that we are using is a\n",
    "`sequence-to-sequence <https://arxiv.org/abs/1409.3215>`__ (seq2seq)\n",
    "model. This type of model is used in cases when our input is a\n",
    "variable-length sequence, and our output is also a variable length\n",
    "sequence that is not necessarily a one-to-one mapping of the input. A\n",
    "seq2seq model is comprised of two recurrent neural networks (RNNs) that\n",
    "work cooperatively: an **encoder** and a **decoder**.\n",
    "\n",
    ".. figure:: /_static/img/chatbot/seq2seq_ts.png\n",
    "   :align: center\n",
    "   :alt: model\n",
    "\n",
    "\n",
    "Image source:\n",
    "https://jeddy92.github.io/JEddy92.github.io/ts_seq2seq_intro/\n",
    "\n",
    "Encoder\n",
    "~~~~~~~\n",
    "\n",
    "The encoder RNN iterates through the input sentence one token\n",
    "(e.g. word) at a time, at each time step outputting an “output” vector\n",
    "and a “hidden state” vector. The hidden state vector is then passed to\n",
    "the next time step, while the output vector is recorded. The encoder\n",
    "transforms the context it saw at each point in the sequence into a set\n",
    "of points in a high-dimensional space, which the decoder will use to\n",
    "generate a meaningful output for the given task.\n",
    "\n",
    "Decoder\n",
    "~~~~~~~\n",
    "\n",
    "The decoder RNN generates the response sentence in a token-by-token\n",
    "fashion. It uses the encoder’s context vectors, and internal hidden\n",
    "states to generate the next word in the sequence. It continues\n",
    "generating words until it outputs an *EOS_token*, representing the end\n",
    "of the sentence. We use an `attention\n",
    "mechanism <https://arxiv.org/abs/1409.0473>`__ in our decoder to help it\n",
    "to “pay attention” to certain parts of the input when generating the\n",
    "output. For our model, we implement `Luong et\n",
    "al. <https://arxiv.org/abs/1508.04025>`__\\ ’s “Global attention” module,\n",
    "and use it as a submodule in our decode model.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Handling\n",
    "-------------\n",
    "\n",
    "Although our models conceptually deal with sequences of tokens, in\n",
    "reality, they deal with numbers like all machine learning models do. In\n",
    "this case, every word in the model’s vocabulary, which was established\n",
    "before training, is mapped to an integer index. We use a ``Voc`` object\n",
    "to contain the mappings from word to index, as well as the total number\n",
    "of words in the vocabulary. We will load the object later before we run\n",
    "the model.\n",
    "\n",
    "Also, in order for us to be able to run evaluations, we must provide a\n",
    "tool for processing our string inputs. The ``normalizeString`` function\n",
    "converts all characters in a string to lowercase and removes all\n",
    "non-letter characters. The ``indexesFromSentence`` function takes a\n",
    "sentence of words and returns the corresponding sequence of word\n",
    "indexes.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Voc:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.trimmed = False\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {PAD_token:\"PAD\", SOS_token:\"SOS\", EOS_token:\"EOS\"}\n",
    "        self.num_words = 3\n",
    "    \n",
    "    def addSentence(self, sentence):\n",
    "        for word in sentence.split(' '):\n",
    "            self.addWord(word)\n",
    "            \n",
    "    def addWord(self, word):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.num_words\n",
    "            self.word2count[word] = 1\n",
    "            self.index2word[self.num_words] = word\n",
    "            self.num_words += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1\n",
    "            \n",
    "    # Remove words below a certain count threshold\n",
    "    def trim(self, min_count):\n",
    "        if self.trimmed:\n",
    "            return\n",
    "        self.trimmed = True\n",
    "        \n",
    "        keep_words = []\n",
    "        \n",
    "        for k, v in self.word2count.items():\n",
    "            if v >= min_count:\n",
    "                keep_words.append(k)\n",
    "                \n",
    "        print('keep_words {} / {} = {:.4f}'.format(\n",
    "            len(keep_words), len(self.word2index), len(keep_words) / len(self.word2index)\n",
    "        ))\n",
    "        \n",
    "        # Reinitialize dictionaries\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {PAD_token:\"PAD\", SOS_token:\"SOS\", EOS_token:\"EOS\"}\n",
    "        self.num_words = 3\n",
    "        \n",
    "        for word in keep_words:\n",
    "            self.addWord(word)\n",
    "\n",
    "# Lowercase, trim, and remove non-letter characters\n",
    "def normalizeString(s):\n",
    "    s = unicodeToAscii(s.lower().strip())\n",
    "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
    "    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
    "    s = re.sub(r\"\\s+\", r\" \", s).strip()\n",
    "    return s\n",
    "\n",
    "def indexesFromSentence(voc, sentence):\n",
    "    return [voc.word2index[word] for word in sentence.split(' ')] + [EOS_token]\n",
    "\n",
    "# Turn a Unicode string to plain ASCII, thanks to\n",
    "# http://stackoverflow.com/a/518232/2809427\n",
    "def unicodeToAscii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD',s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define Encoder\n",
    "--------------\n",
    "\n",
    "We implement our encoder’s RNN with the ``torch.nn.GRU`` module which we\n",
    "feed a batch of sentences (vectors of word embeddings) and it internally\n",
    "iterates through the sentences one token at a time calculating the\n",
    "hidden states. We initialize this module to be bidirectional, meaning\n",
    "that we have two independent GRUs: one that iterates through the\n",
    "sequences in chronological order, and another that iterates in reverse\n",
    "order. We ultimately return the sum of these two GRUs’ outputs. Since\n",
    "our model was trained using batching, our ``EncoderRNN`` model’s\n",
    "``forward`` function expects a padded input batch. To batch\n",
    "variable-length sentences, we allow a maximum of *MAX_LENGTH* tokens in\n",
    "a sentence, and all sentences in the batch that have less than\n",
    "*MAX_LENGTH* tokens are padded at the end with our dedicated *PAD_token*\n",
    "tokens. To use padded batches with a PyTorch RNN module, we must wrap\n",
    "the forward pass call with ``torch.nn.utils.rnn.pack_padded_sequence``\n",
    "and ``torch.nn.utils.rnn.pad_packed_sequence`` data transformations.\n",
    "Note that the ``forward`` function also takes an ``input_lengths`` list,\n",
    "which contains the length of each sentence in the batch. This input is\n",
    "used by the ``torch.nn.utils.rnn.pack_padded_sequence`` function when\n",
    "padding.\n",
    "\n",
    "Hybrid Frontend Notes:\n",
    "~~~~~~~~~~~~~~~~~~~~~~\n",
    "\n",
    "Since the encoder’s ``forward`` function does not contain any\n",
    "data-dependent control flow, we will use **tracing** to convert it to\n",
    "script mode. When tracing a module, we can leave the module definition\n",
    "as-is. We will initialize all models towards the end of this document\n",
    "before we run evaluations.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, embedding, n_layers=1, dropout=0):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = embedding\n",
    "        \n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, n_layers,\n",
    "                          dropout=(0 if n_layers == 1 else dropout), bidirectional=True)\n",
    "        \n",
    "        \n",
    "    def forward(self, input_seq, input_lengths, hidden=None):\n",
    "        embedded = self.embedding(input_seq)\n",
    "        packed = torch.nn.utils.rnn.pack_padded_sequence(embedded, input_lengths)\n",
    "        outputs, hidden = self.gru(packed, hidden)\n",
    "        outputs, _ = torch.nn.utils.rnn.pad_packed_sequence(outputs)\n",
    "        outputs = outputs[:, :, :self.hidden_size] + outputs[:, :, self.hidden_size:]\n",
    "        return outputs, hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define Decoder’s Attention Module\n",
    "---------------------------------\n",
    "\n",
    "Next, we’ll define our attention module (``Attn``). Note that this\n",
    "module will be used as a submodule in our decoder model. Luong et\n",
    "al. consider various “score functions”, which take the current decoder\n",
    "RNN output and the entire encoder output, and return attention\n",
    "“energies”. This attention energies tensor is the same size as the\n",
    "encoder output, and the two are ultimately multiplied, resulting in a\n",
    "weighted tensor whose largest values represent the most important parts\n",
    "of the query sentence at a particular time-step of decoding.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Luong attention layer\n",
    "class Attn(torch.nn.Module):\n",
    "    def __init__(self, method, hidden_size):\n",
    "        super(Attn, self).__init__()\n",
    "        self.method = method\n",
    "        if self.method not in ['dot', 'general', 'concat']:\n",
    "            raise ValueError(self.method, \"is not an appropriate attention method.\")\n",
    "        self.hidden_size = hidden_size\n",
    "        if self.method == 'general':\n",
    "            self.attn = torch.nn.Linear(self.hidden_size, hidden_size)\n",
    "        elif self.method == 'concat':\n",
    "            self.attn = torch.nn.Linear(self.hidden_size * 2, hidden_size)\n",
    "            self.v = torch.nn.Parameter(torch.FloatTensor(hidden_size))\n",
    "        \n",
    "    def dot_score(self, hidden, encoder_output):\n",
    "        return torch.sum(hidden * encoder_output, dim = 2)\n",
    "    \n",
    "\n",
    "    def general_score(self, hidden, encoder_output):\n",
    "        energy = self.attn(encoder_output)\n",
    "        return torch.sum(hidden * energy, dim = 2)\n",
    "    \n",
    "    def concat_score(self, hidden, encoder_output):\n",
    "        energy = self.attn(torch.cat((hidden.expand(encoder_output.size(0), -1, -1),\n",
    "                                     encoder_output), 2)).tanh()\n",
    "        return torch.sum(self.v * energy, dim=2)\n",
    "    \n",
    "    def forward(self, hidden, encoder_outputs):\n",
    "        # Calculate the attention weights (energies) based on the given method\n",
    "        if self.method == 'general':\n",
    "            attn_energies = self.general_score(hidden, encoder_outputs)\n",
    "        elif self.method == 'concat':\n",
    "            attn_energies = self.concat_score(hidden, encoder_outputs)\n",
    "        elif self.method == 'dot':\n",
    "            attn_energies = self.dot_score(hidden, encoder_outputs)\n",
    "            \n",
    "        attn_energies = attn_energies.t()\n",
    "        \n",
    "        return F.softmax(attn_energies, dim=1).unsqueeze(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define Decoder\n",
    "--------------\n",
    "\n",
    "Similarly to the ``EncoderRNN``, we use the ``torch.nn.GRU`` module for\n",
    "our decoder’s RNN. This time, however, we use a unidirectional GRU. It\n",
    "is important to note that unlike the encoder, we will feed the decoder\n",
    "RNN one word at a time. We start by getting the embedding of the current\n",
    "word and applying a\n",
    "`dropout <https://pytorch.org/docs/stable/nn.html?highlight=dropout#torch.nn.Dropout>`__.\n",
    "Next, we forward the embedding and the last hidden state to the GRU and\n",
    "obtain a current GRU output and hidden state. We then use our ``Attn``\n",
    "module as a layer to obtain the attention weights, which we multiply by\n",
    "the encoder’s output to obtain our attended encoder output. We use this\n",
    "attended encoder output as our ``context`` tensor, which represents a\n",
    "weighted sum indicating what parts of the encoder’s output to pay\n",
    "attention to. From here, we use a linear layer and softmax normalization\n",
    "to select the next word in the output sequence.\n",
    "\n",
    "Hybrid Frontend Notes:\n",
    "~~~~~~~~~~~~~~~~~~~~~~\n",
    "\n",
    "Similarly to the ``EncoderRNN``, this module does not contain any\n",
    "data-dependent control flow. Therefore, we can once again use\n",
    "**tracing** to convert this model to Torch Script after it is\n",
    "initialized and its parameters are loaded.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LuongAttnDecoderRNN(nn.Module):\n",
    "    def __init__(self, attn_model, embedding, hidden_size, output_size, n_layers=1, dropout=0.1):\n",
    "        super(LuongAttnDecoderRNN, self).__init__()\n",
    "        \n",
    "        # keep for reference\n",
    "        self.attn_model = attn_model\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        self.dropout = dropout\n",
    "        \n",
    "        # Define layers\n",
    "        self.embedding = embedding\n",
    "        self.embedding_dropout = nn.Dropout(dropout)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, n_layers, dropout=(0 if n_layers==1 else dropout))\n",
    "        self.concat = nn.Linear(hidden_size * 2, hidden_size)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "        \n",
    "        self.attn = Attn(attn_model, hidden_size)\n",
    "        \n",
    "    def forward(self, input_step, last_hidden, encoder_outputs):\n",
    "        \n",
    "        embedded = self.embedding(input_step)\n",
    "        embedded = self.embedding_dropout(embedded)\n",
    "        \n",
    "        rnn_output, hidden = self.gru(embedded, last_hidden)\n",
    "        \n",
    "        attn_weights = self.attn(rnn_output, encoder_outputs)\n",
    "        \n",
    "        context = attn_weights.bmm(encoder_outputs.transpose(0, 1))\n",
    "        rnn_output = rnn_output.squeeze(0)\n",
    "        context = context.squeeze(1)\n",
    "        concat_input = torch.cat((rnn_output, context), 1)\n",
    "        concat_output = torch.tanh(self.concat(concat_input))\n",
    "        output = self.out(concat_output)\n",
    "        output = F.softmax(output, dim=1)\n",
    "        return output, hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define Evaluation\n",
    "-----------------\n",
    "\n",
    "Greedy Search Decoder\n",
    "~~~~~~~~~~~~~~~~~~~~~\n",
    "\n",
    "As in the chatbot tutorial, we use a ``GreedySearchDecoder`` module to\n",
    "facilitate the actual decoding process. This module has the trained\n",
    "encoder and decoder models as attributes, and drives the process of\n",
    "encoding an input sentence (a vector of word indexes), and iteratively\n",
    "decoding an output response sequence one word (word index) at a time.\n",
    "\n",
    "Encoding the input sequence is straightforward: simply forward the\n",
    "entire sequence tensor and its corresponding lengths vector to the\n",
    "``encoder``. It is important to note that this module only deals with\n",
    "one input sequence at a time, **NOT** batches of sequences. Therefore,\n",
    "when the constant **1** is used for declaring tensor sizes, this\n",
    "corresponds to a batch size of 1. To decode a given decoder output, we\n",
    "must iteratively run forward passes through our decoder model, which\n",
    "outputs softmax scores corresponding to the probability of each word\n",
    "being the correct next word in the decoded sequence. We initialize the\n",
    "``decoder_input`` to a tensor containing an *SOS_token*. After each pass\n",
    "through the ``decoder``, we *greedily* append the word with the highest\n",
    "softmax probability to the ``decoded_words`` list. We also use this word\n",
    "as the ``decoder_input`` for the next iteration. The decoding process\n",
    "terminates either if the ``decoded_words`` list has reached a length of\n",
    "*MAX_LENGTH* or if the predicted word is the *EOS_token*.\n",
    "\n",
    "Hybrid Frontend Notes:\n",
    "~~~~~~~~~~~~~~~~~~~~~~\n",
    "\n",
    "The ``forward`` method of this module involves iterating over the range\n",
    "of $[0, max\\_length)$ when decoding an output sequence one word at\n",
    "a time. Because of this, we should use **scripting** to convert this\n",
    "module to Torch Script. Unlike with our encoder and decoder models,\n",
    "which we can trace, we must make some necessary changes to the\n",
    "``GreedySearchDecoder`` module in order to initialize an object without\n",
    "error. In other words, we must ensure that our module adheres to the\n",
    "rules of the scripting mechanism, and does not utilize any language\n",
    "features outside of the subset of Python that Torch Script includes.\n",
    "\n",
    "To get an idea of some manipulations that may be required, we will go\n",
    "over the diffs between the ``GreedySearchDecoder`` implementation from\n",
    "the chatbot tutorial and the implementation that we use in the cell\n",
    "below. Note that the lines highlighted in red are lines removed from the\n",
    "original implementation and the lines highlighted in green are new.\n",
    "\n",
    ".. figure:: /_static/img/chatbot/diff.png\n",
    "   :align: center\n",
    "   :alt: diff\n",
    "\n",
    "Changes:\n",
    "^^^^^^^^\n",
    "\n",
    "-  ``nn.Module`` -> ``torch.jit.ScriptModule``\n",
    "\n",
    "   -  In order to use PyTorch’s scripting mechanism on a module, that\n",
    "      module must inherit from the ``torch.jit.ScriptModule``.\n",
    "\n",
    "\n",
    "-  Added ``decoder_n_layers`` to the constructor arguments\n",
    "\n",
    "   -  This change stems from the fact that the encoder and decoder\n",
    "      models that we pass to this module will be a child of\n",
    "      ``TracedModule`` (not ``Module``). Therefore, we cannot access the\n",
    "      decoder’s number of layers with ``decoder.n_layers``. Instead, we\n",
    "      plan for this, and pass this value in during module construction.\n",
    "\n",
    "\n",
    "-  Store away new attributes as constants\n",
    "\n",
    "   -  In the original implementation, we were free to use variables from\n",
    "      the surrounding (global) scope in our ``GreedySearchDecoder``\\ ’s\n",
    "      ``forward`` method. However, now that we are using scripting, we\n",
    "      do not have this freedom, as the assumption with scripting is that\n",
    "      we cannot necessarily hold on to Python objects, especially when\n",
    "      exporting. An easy solution to this is to store these values from\n",
    "      the global scope as attributes to the module in the constructor,\n",
    "      and add them to a special list called ``__constants__`` so that\n",
    "      they can be used as literal values when constructing the graph in\n",
    "      the ``forward`` method. An example of this usage is on NEW line\n",
    "      19, where instead of using the ``device`` and ``SOS_token`` global\n",
    "      values, we use our constant attributes ``self._device`` and\n",
    "      ``self._SOS_token``.\n",
    "\n",
    "\n",
    "-  Add the ``torch.jit.script_method`` decorator to the ``forward``\n",
    "   method\n",
    "\n",
    "   -  Adding this decorator lets the JIT compiler know that the function\n",
    "      that it is decorating should be scripted.\n",
    "\n",
    "\n",
    "-  Enforce types of ``forward`` method arguments\n",
    "\n",
    "   -  By default, all parameters to a Torch Script function are assumed\n",
    "      to be Tensor. If we need to pass an argument of a different type,\n",
    "      we can use function type annotations as introduced in `PEP\n",
    "      3107 <https://www.python.org/dev/peps/pep-3107/>`__. In addition,\n",
    "      it is possible to declare arguments of different types using\n",
    "      MyPy-style type annotations (see\n",
    "      `doc <https://pytorch.org/docs/master/jit.html#types>`__).\n",
    "\n",
    "\n",
    "-  Change initialization of ``decoder_input``\n",
    "\n",
    "   -  In the original implementation, we initialized our\n",
    "      ``decoder_input`` tensor with ``torch.LongTensor([[SOS_token]])``.\n",
    "      When scripting, we are not allowed to initialize tensors in a\n",
    "      literal fashion like this. Instead, we can initialize our tensor\n",
    "      with an explicit torch function such as ``torch.ones``. In this\n",
    "      case, we can easily replicate the scalar ``decoder_input`` tensor\n",
    "      by multiplying 1 by our SOS_token value stored in the constant\n",
    "      ``self._SOS_token``.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class GreedySearchDecoder(torch.jit.ScriptModule):\n",
    "    def __init__(self, encoder, decoder, decoder_n_layers):\n",
    "        super(GreedySearchDecoder, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self._device = device\n",
    "        self._SOS_token = SOS_token\n",
    "        self._decoder_n_layers = decoder_n_layers\n",
    "\n",
    "    __constants__ = ['_device', '_SOS_token', '_decoder_n_layers']        \n",
    "\n",
    "    @torch.jit.script_method\n",
    "    def forward(self, input_seq : torch.Tensor, input_length : torch.Tensor, max_length : int):\n",
    "        # Forward input through encoder model\n",
    "        encoder_outputs, encoder_hidden = self.encoder(input_seq, input_length)\n",
    "        # Prepare encoder's final hidden layer to be first hidden input to the decoder\n",
    "        decoder_hidden = encoder_hidden[:self._decoder_n_layers]\n",
    "        # Initialize decoder input with SOS_token\n",
    "        decoder_input = torch.ones(1, 1, device=self._device, dtype=torch.long) * self._SOS_token\n",
    "        # Initialize tensors to append decoded words to\n",
    "        all_tokens = torch.zeros([0], device=self._device, dtype=torch.long)\n",
    "        all_scores = torch.zeros([0], device=self._device)\n",
    "        # Iteratively decode one word token at a time\n",
    "        for _ in range(max_length):\n",
    "            # Forward pass through decoder\n",
    "            decoder_output, decoder_hidden = self.decoder(decoder_input, decoder_hidden, encoder_outputs)\n",
    "            # Obtain most likely word token and its softmax score\n",
    "            decoder_scores, decoder_input = torch.max(decoder_output, dim=1)\n",
    "            # Record token and score\n",
    "            all_tokens = torch.cat((all_tokens, decoder_input), dim=0)\n",
    "            all_scores = torch.cat((all_scores, decoder_scores), dim=0)\n",
    "            # Prepare current token to be next decoder input (add a dimension)\n",
    "            decoder_input = torch.unsqueeze(decoder_input, 0)\n",
    "        # Return collections of word tokens and scores\n",
    "        return all_tokens, all_scores "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Evaluating an Input**\n",
    "\n",
    "\n",
    "Next, we define some functions for evaluating an input. The ``evaluate``\n",
    "function takes a normalized string sentence, processes it to a tensor of\n",
    "its corresponding word indexes (with batch size of 1), and passes this\n",
    "tensor to a ``GreedySearchDecoder`` instance called ``searcher`` to\n",
    "handle the encoding/decoding process. The searcher returns the output\n",
    "word index vector and a scores tensor corresponding to the softmax\n",
    "scores for each decoded word token. The final step is to convert each\n",
    "word index back to its string representation using ``voc.index2word``.\n",
    "\n",
    "We also define two functions for evaluating an input sentence. The\n",
    "``evaluateInput`` function prompts a user for an input, and evaluates\n",
    "it. It will continue to ask for another input until the user enters ‘q’\n",
    "or ‘quit’.\n",
    "\n",
    "The ``evaluateExample`` function simply takes a string input sentence as\n",
    "an argument, normalizes it, evaluates it, and prints the response.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluate(encoder, decoder, searcher, voc, sentence, max_length=MAX_LENGTH):\n",
    "    indexes_batch = [indexesFromSentence(voc, sentence)]\n",
    "    lengths = torch.tensor([len(indexes) for indexes in indexes_batch])\n",
    "    input_batch = torch.LongTensor(indexes_batch).transpose(0, 1)\n",
    "    input_batch = input_batch.to(device)\n",
    "    lengths = lengths.to(device)\n",
    "    tokens, scores = searcher(input_batch, lengths, max_length)\n",
    "    decoded_words = [voc.index2word[token.item()] for token in tokens]\n",
    "    return decoded_words\n",
    "\n",
    "def evaluateInput(encoder, decoder, searcher, voc):\n",
    "    input_sentence = ''\n",
    "    while(1):\n",
    "        try:\n",
    "            # Get input sentence\n",
    "            input_sentence = input('> ')\n",
    "            if input_sentence == 'q' or input_sentence == 'quit': break\n",
    "            input_sentence = normalizeString(input_sentence)\n",
    "            output_words = evaluate(encoder, decoder, searcher, voc, input_sentence)\n",
    "            output_words[:] = [x for x in output_words if not (x == \"EOS\" or x == \"PAD\")]\n",
    "            print('Bot', ' '.join(output_words))\n",
    "        except KeyError:\n",
    "            print(\"Error: Encountered unknown word.\")\n",
    "            \n",
    "# Normalize input sentence and call evaluate()\n",
    "def evaluateExample(sentence, encoder, decoder, searcher, voc):\n",
    "    print('> ' + sentence)\n",
    "    input_sentence = normalizeString(sentence)\n",
    "    output_words = evaluate(encoder, decoder, searcher, voc, input_sentence)\n",
    "    output_words[:] = [x for x in output_words if not (x == \"EOS\" or x == \"PAD\")]\n",
    "    print('Bot', ' '.join(output_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Pretrained Parameters\n",
    "--------------------------\n",
    "\n",
    "Ok, its time to load our model!\n",
    "\n",
    "Use hosted model\n",
    "~~~~~~~~~~~~~~~~\n",
    "\n",
    "To load the hosted model:\n",
    "\n",
    "1) Download the model `here <https://download.pytorch.org/models/tutorials/4000_checkpoint.tar>`__.\n",
    "\n",
    "2) Set the ``loadFilename`` variable to the path to the downloaded\n",
    "   checkpoint file.\n",
    "\n",
    "3) Leave the ``checkpoint = torch.load(loadFilename)`` line uncommented,\n",
    "   as the hosted model was trained on CPU.\n",
    "\n",
    "Use your own model\n",
    "~~~~~~~~~~~~~~~~~~\n",
    "\n",
    "To load your own pre-trained model:\n",
    "\n",
    "1) Set the ``loadFilename`` variable to the path to the checkpoint file\n",
    "   that you wish to load. Note that if you followed the convention for\n",
    "   saving the model from the chatbot tutorial, this may involve changing\n",
    "   the ``model_name``, ``encoder_n_layers``, ``decoder_n_layers``,\n",
    "   ``hidden_size``, and ``checkpoint_iter`` (as these values are used in\n",
    "   the model path).\n",
    "\n",
    "2) If you trained the model on a CPU, make sure that you are opening the\n",
    "   checkpoint with the ``checkpoint = torch.load(loadFilename)`` line.\n",
    "   If you trained the model on a GPU and are running this tutorial on a\n",
    "   CPU, uncomment the\n",
    "   ``checkpoint = torch.load(loadFilename, map_location=torch.device('cpu'))``\n",
    "   line.\n",
    "\n",
    "Hybrid Frontend Notes:\n",
    "~~~~~~~~~~~~~~~~~~~~~~\n",
    "\n",
    "Notice that we initialize and load parameters into our encoder and\n",
    "decoder models as usual. Also, we must call ``.to(device)`` to set the\n",
    "device options of the models and ``.eval()`` to set the dropout layers\n",
    "to test mode **before** we trace the models. ``TracedModule`` objects do\n",
    "not inherit the ``to`` or ``eval`` methods.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building encoder and decoder ...\n",
      "Models built and ready to go!\n"
     ]
    }
   ],
   "source": [
    "save_dir = os.path.join(\"data\", \"save\")\n",
    "corpus_name = \"cornell movie-dialogs corpus\"\n",
    "\n",
    "# Configure models\n",
    "model_name = 'cb_model'\n",
    "attn_model = 'dot'\n",
    "#attn_model = 'general'\n",
    "#attn_model = 'concat'\n",
    "hidden_size = 500\n",
    "encoder_n_layers = 2\n",
    "decoder_n_layers = 2\n",
    "dropout = 0.1\n",
    "batch_size = 64\n",
    "\n",
    "loadFilename = 'data/5_checkpoint.tar'\n",
    "\n",
    "\n",
    "# Load model\n",
    "# Force CPU devices options\n",
    "checkpoint = torch.load(loadFilename, map_location=torch.device('cpu'))\n",
    "encoder_sd = checkpoint['en']\n",
    "decoder_sd = checkpoint['de']\n",
    "encoder_optimizer_sd = checkpoint['en_opt']\n",
    "decoder_optimizer_sd = checkpoint['de_opt']\n",
    "embedding_sd = checkpoint['embedding']\n",
    "voc = Voc(corpus_name)\n",
    "voc.__dict__ = checkpoint['voc_dict']\n",
    "\n",
    "print('Building encoder and decoder ...')\n",
    "embedding = nn.Embedding(voc.num_words, hidden_size)\n",
    "embedding.load_state_dict(embedding_sd)\n",
    "\n",
    "encoder = EncoderRNN(hidden_size, embedding, encoder_n_layers, dropout)\n",
    "decoder = LuongAttnDecoderRNN(attn_model, embedding, hidden_size, voc.num_words,\n",
    "                              decoder_n_layers, dropout)\n",
    "\n",
    "encoder.load_state_dict(encoder_sd)\n",
    "decoder.load_state_dict(decoder_sd)\n",
    "    \n",
    "encoder = encoder.to(device)\n",
    "decoder = decoder.to(device)\n",
    "\n",
    "encoder.eval()\n",
    "decoder.eval()\n",
    "print('Models built and ready to go!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert Model to Torch Script\n",
    "-----------------------------\n",
    "\n",
    "Encoder\n",
    "~~~~~~~\n",
    "\n",
    "As previously mentioned, to convert the encoder model to Torch Script,\n",
    "we use **tracing**. Tracing any module requires running an example input\n",
    "through the model’s ``forward`` method and trace the computational graph\n",
    "that the data encounters. The encoder model takes an input sequence and\n",
    "a corresponding lengths tensor. Therefore, we create an example input\n",
    "sequence tensor ``test_seq``, which is of appropriate size (MAX_LENGTH,\n",
    "1), contains numbers in the appropriate range\n",
    "$[0, voc.num\\_words)$, and is of the appropriate type (int64). We\n",
    "also create a ``test_seq_length`` scalar which realistically contains\n",
    "the value corresponding to how many words are in the ``test_seq``. The\n",
    "next step is to use the ``torch.jit.trace`` function to trace the model.\n",
    "Notice that the first argument we pass is the module that we want to\n",
    "trace, and the second is a tuple of arguments to the module’s\n",
    "``forward`` method.\n",
    "\n",
    "Decoder\n",
    "~~~~~~~\n",
    "\n",
    "We perform the same process for tracing the decoder as we did for the\n",
    "encoder. Notice that we call forward on a set of random inputs to the\n",
    "traced_encoder to get the output that we need for the decoder. This is\n",
    "not required, as we could also simply manufacture a tensor of the\n",
    "correct shape, type, and value range. This method is possible because in\n",
    "our case we do not have any constraints on the values of the tensors\n",
    "because we do not have any operations that could fault on out-of-range\n",
    "inputs.\n",
    "\n",
    "GreedySearchDecoder\n",
    "~~~~~~~~~~~~~~~~~~~\n",
    "\n",
    "Recall that we scripted our searcher module due to the presence of\n",
    "data-dependent control flow. In the case of scripting, we do the\n",
    "conversion work up front by adding the decorator and making sure the\n",
    "implementation complies with scripting rules. We initialize the scripted\n",
    "searcher the same way that we would initialize an un-scripted variant.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Convert encoder model\n",
    "# Create artificial inputs\n",
    "test_seq = torch.LongTensor(MAX_LENGTH, 1).random_(0, voc.num_words)\n",
    "test_seq_length = torch.LongTensor([test_seq.size()[0]])\n",
    "# Trace the model\n",
    "traced_encoder = torch.jit.trace(encoder, (test_seq, test_seq_length))\n",
    "\n",
    "# Convert decoder model\n",
    "test_encoder_outputs, test_encoder_hidden = traced_encoder(test_seq, test_seq_length)\n",
    "test_decoder_hidden = test_encoder_hidden[:decoder.n_layers]\n",
    "test_decoder_input = torch.LongTensor(1, 1).random_(0, voc.num_words)\n",
    "# Trace the model\n",
    "traced_decoder = torch.jit.trace(decoder, (test_decoder_input, test_decoder_hidden, test_encoder_outputs))\n",
    "\n",
    "# Initialize searcher module\n",
    "scripted_searcher = GreedySearchDecoder(traced_encoder, traced_decoder, decoder.n_layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print Graphs\n",
    "------------\n",
    "\n",
    "Now that our models are in Torch Script form, we can print the graphs of\n",
    "each to ensure that we captured the computational graph appropriately.\n",
    "Since our ``scripted_searcher`` contains our ``traced_encoder`` and\n",
    "``traced_decoder``, these graphs will print inline.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scripted_searcher graph:\n",
      " graph(%input_seq : Tensor\n",
      "      %input_length : Tensor\n",
      "      %max_length : int\n",
      "      %3 : Tensor\n",
      "      %4 : Tensor\n",
      "      %5 : Tensor\n",
      "      %6 : Tensor\n",
      "      %7 : Tensor\n",
      "      %8 : Tensor\n",
      "      %9 : Tensor\n",
      "      %10 : Tensor\n",
      "      %11 : Tensor\n",
      "      %12 : Tensor\n",
      "      %13 : Tensor\n",
      "      %14 : Tensor\n",
      "      %15 : Tensor\n",
      "      %16 : Tensor\n",
      "      %17 : Tensor\n",
      "      %18 : Tensor\n",
      "      %19 : Tensor\n",
      "      %118 : Tensor\n",
      "      %119 : Tensor\n",
      "      %120 : Tensor\n",
      "      %121 : Tensor\n",
      "      %122 : Tensor\n",
      "      %123 : Tensor\n",
      "      %124 : Tensor\n",
      "      %125 : Tensor\n",
      "      %126 : Tensor\n",
      "      %127 : Tensor\n",
      "      %128 : Tensor\n",
      "      %129 : Tensor\n",
      "      %130 : Tensor) {\n",
      "  %58 : int = prim::Constant[value=9223372036854775807](), scope: EncoderRNN\n",
      "  %53 : float = prim::Constant[value=0](), scope: EncoderRNN\n",
      "  %43 : float = prim::Constant[value=0.1](), scope: EncoderRNN/GRU[gru]\n",
      "  %42 : int = prim::Constant[value=2](), scope: EncoderRNN/GRU[gru]\n",
      "  %41 : bool = prim::Constant[value=1](), scope: EncoderRNN/GRU[gru]\n",
      "  %36 : int = prim::Constant[value=6](), scope: EncoderRNN/GRU[gru]\n",
      "  %34 : int = prim::Constant[value=500](), scope: EncoderRNN/GRU[gru]\n",
      "  %25 : int = prim::Constant[value=4](), scope: EncoderRNN\n",
      "  %24 : Device = prim::Constant[value=\"cpu\"](), scope: EncoderRNN\n",
      "  %21 : bool = prim::Constant[value=0](), scope: EncoderRNN/Embedding[embedding]\n",
      "  %20 : int = prim::Constant[value=-1](), scope: EncoderRNN/Embedding[embedding]\n",
      "  %90 : int = prim::Constant[value=0]()\n",
      "  %94 : int = prim::Constant[value=1]()\n",
      "  %input.7 : Float(10, 1, 500) = aten::embedding(%3, %input_seq, %20, %21, %21), scope: EncoderRNN/Embedding[embedding]\n",
      "  %lengths : Long(1) = aten::to(%input_length, %24, %25, %21, %21), scope: EncoderRNN\n",
      "  %input.1 : Float(10, 500), %batch_sizes : Long(10) = aten::_pack_padded_sequence(%input.7, %lengths, %21), scope: EncoderRNN\n",
      "  %35 : int[] = prim::ListConstruct(%25, %94, %34), scope: EncoderRNN/GRU[gru]\n",
      "  %hx : Float(4, 1, 500) = aten::zeros(%35, %36, %90, %24), scope: EncoderRNN/GRU[gru]\n",
      "  %40 : Tensor[] = prim::ListConstruct(%4, %5, %6, %7, %8, %9, %10, %11, %12, %13, %14, %15, %16, %17, %18, %19), scope: EncoderRNN/GRU[gru]\n",
      "  %46 : Float(10, 1000), %encoder_hidden : Float(4, 1, 500) = aten::gru(%input.1, %batch_sizes, %hx, %40, %41, %42, %43, %21, %41), scope: EncoderRNN/GRU[gru]\n",
      "  %49 : int = aten::size(%batch_sizes, %90), scope: EncoderRNN\n",
      "  %max_seq_length : Long() = prim::NumToTensor(%49), scope: EncoderRNN\n",
      "  %51 : int = prim::TensorToNum(%max_seq_length), scope: EncoderRNN\n",
      "  %outputs : Float(10, 1, 1000), %55 : Long(1) = aten::_pad_packed_sequence(%46, %batch_sizes, %21, %53, %51), scope: EncoderRNN\n",
      "  %60 : Float(10, 1, 1000) = aten::slice(%outputs, %90, %90, %58, %94), scope: EncoderRNN\n",
      "  %65 : Float(10, 1, 1000) = aten::slice(%60, %94, %90, %58, %94), scope: EncoderRNN\n",
      "  %70 : Float(10, 1!, 500) = aten::slice(%65, %42, %90, %34, %94), scope: EncoderRNN\n",
      "  %75 : Float(10, 1, 1000) = aten::slice(%outputs, %90, %90, %58, %94), scope: EncoderRNN\n",
      "  %80 : Float(10, 1, 1000) = aten::slice(%75, %94, %90, %58, %94), scope: EncoderRNN\n",
      "  %85 : Float(10, 1!, 500) = aten::slice(%80, %42, %34, %58, %94), scope: EncoderRNN\n",
      "  %encoder_outputs : Float(10, 1, 500) = aten::add(%70, %85, %94), scope: EncoderRNN\n",
      "  %decoder_hidden.1 : Tensor = aten::slice(%encoder_hidden, %90, %90, %42, %94)\n",
      "  %98 : int[] = prim::ListConstruct(%94, %94)\n",
      "  %100 : Tensor = aten::ones(%98, %25, %90, %24)\n",
      "  %decoder_input.1 : Tensor = aten::mul(%100, %94)\n",
      "  %103 : int[] = prim::ListConstruct(%90)\n",
      "  %all_tokens.1 : Tensor = aten::zeros(%103, %25, %90, %24)\n",
      "  %108 : int[] = prim::ListConstruct(%90)\n",
      "  %all_scores.1 : Tensor = aten::zeros(%108, %36, %90, %24)\n",
      "  %all_scores : Tensor, %all_tokens : Tensor, %decoder_hidden : Tensor, %decoder_input : Tensor = prim::Loop(%max_length, %41, %all_scores.1, %all_tokens.1, %decoder_hidden.1, %decoder_input.1)\n",
      "    block0(%114 : int, %188 : Tensor, %184 : Tensor, %116 : Tensor, %115 : Tensor) {\n",
      "      %input.2 : Float(1, 1, 500) = aten::embedding(%118, %115, %20, %21, %21), scope: LuongAttnDecoderRNN/Embedding[embedding]\n",
      "      %input.3 : Float(1, 1, 500) = aten::dropout(%input.2, %43, %21), scope: LuongAttnDecoderRNN/Dropout[embedding_dropout]\n",
      "      %138 : Tensor[] = prim::ListConstruct(%119, %120, %121, %122, %123, %124, %125, %126), scope: LuongAttnDecoderRNN/GRU[gru]\n",
      "      %hidden : Float(1, 1, 500), %decoder_hidden.2 : Float(2, 1, 500) = aten::gru(%input.3, %116, %138, %41, %42, %43, %21, %21, %21), scope: LuongAttnDecoderRNN/GRU[gru]\n",
      "      %147 : Float(10, 1, 500) = aten::mul(%hidden, %encoder_outputs), scope: LuongAttnDecoderRNN/Attn[attn]\n",
      "      %149 : int[] = prim::ListConstruct(%42), scope: LuongAttnDecoderRNN/Attn[attn]\n",
      "      %attn_energies : Float(10, 1) = aten::sum(%147, %149, %21), scope: LuongAttnDecoderRNN/Attn[attn]\n",
      "      %input.4 : Float(1!, 10) = aten::t(%attn_energies), scope: LuongAttnDecoderRNN/Attn[attn]\n",
      "      %154 : Float(1, 10) = aten::softmax(%input.4, %94), scope: LuongAttnDecoderRNN/Attn[attn]\n",
      "      %attn_weights : Float(1, 1, 10) = aten::unsqueeze(%154, %94), scope: LuongAttnDecoderRNN/Attn[attn]\n",
      "      %159 : Float(1!, 10, 500) = aten::transpose(%encoder_outputs, %90, %94), scope: LuongAttnDecoderRNN\n",
      "      %context.1 : Float(1, 1, 500) = aten::bmm(%attn_weights, %159), scope: LuongAttnDecoderRNN\n",
      "      %rnn_output : Float(1, 500) = aten::squeeze(%hidden, %90), scope: LuongAttnDecoderRNN\n",
      "      %context : Float(1, 500) = aten::squeeze(%context.1, %94), scope: LuongAttnDecoderRNN\n",
      "      %165 : Tensor[] = prim::ListConstruct(%rnn_output, %context), scope: LuongAttnDecoderRNN\n",
      "      %input.5 : Float(1, 1000) = aten::cat(%165, %94), scope: LuongAttnDecoderRNN\n",
      "      %168 : Float(1000!, 500!) = aten::t(%127), scope: LuongAttnDecoderRNN/Linear[concat]\n",
      "      %171 : Float(1, 500) = aten::addmm(%128, %input.5, %168, %94, %94), scope: LuongAttnDecoderRNN/Linear[concat]\n",
      "      %input.6 : Float(1, 500) = aten::tanh(%171), scope: LuongAttnDecoderRNN\n",
      "      %173 : Float(500!, 7826!) = aten::t(%129), scope: LuongAttnDecoderRNN/Linear[out]\n",
      "      %input : Float(1, 7826) = aten::addmm(%130, %input.6, %173, %94, %94), scope: LuongAttnDecoderRNN/Linear[out]\n",
      "      %decoder_output : Float(1, 7826) = aten::softmax(%input, %94), scope: LuongAttnDecoderRNN\n",
      "      %decoder_scores : Tensor, %decoder_input.2 : Tensor = aten::max(%decoder_output, %94, %21)\n",
      "      %186 : Tensor[] = prim::ListConstruct(%184, %decoder_input.2)\n",
      "      %all_tokens.2 : Tensor = aten::cat(%186, %90)\n",
      "      %190 : Tensor[] = prim::ListConstruct(%188, %decoder_scores)\n",
      "      %all_scores.2 : Tensor = aten::cat(%190, %90)\n",
      "      %decoder_input.3 : Tensor = aten::unsqueeze(%decoder_input.2, %90)\n",
      "      -> (%41, %all_scores.2, %all_tokens.2, %decoder_hidden.2, %decoder_input.3)\n",
      "    }\n",
      "  return (%all_tokens, %all_scores);\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('scripted_searcher graph:\\n', scripted_searcher.graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run Evaluation\n",
    "--------------\n",
    "\n",
    "Finally, we will run evaluation of the chatbot model using the Torch\n",
    "Script models. If converted correctly, the models will behave exactly as\n",
    "they would in their eager-mode representation.\n",
    "\n",
    "By default, we evaluate a few common query sentences. If you want to\n",
    "chat with the bot yourself, uncomment the ``evaluateInput`` line and\n",
    "give it a spin.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> hello\n",
      "Bot \n",
      "> what's up?\n",
      "Bot \n",
      "> who are you?\n",
      "Bot \n",
      "> where am I?\n",
      "Bot \n",
      "> where are you from?\n",
      "Bot \n",
      "> quit\n"
     ]
    }
   ],
   "source": [
    "sentences = [\"hello\", \"what's up?\", \"who are you?\", \"where am I?\", \"where are you from?\"]\n",
    "for s in sentences:\n",
    "    evaluateExample(s, traced_encoder, traced_decoder, scripted_searcher, voc)\n",
    "    \n",
    "evaluateInput(traced_encoder, traced_decoder, scripted_searcher, voc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save Model\n",
    "----------\n",
    "\n",
    "Now that we have successfully converted our model to Torch Script, we\n",
    "will serialize it for use in a non-Python deployment environment. To do\n",
    "this, we can simply save our ``scripted_searcher`` module, as this is\n",
    "the user-facing interface for running inference against the chatbot\n",
    "model. When saving a Script module, use script_module.save(PATH) instead\n",
    "of torch.save(model, PATH).\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scripted_searcher.save(\"scripted_chatbot.pth\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
